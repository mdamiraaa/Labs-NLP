{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_NLP_Torch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwF3ZkmW3gAy",
        "colab_type": "code",
        "outputId": "81f55749-f966-4740-e677-eb5fa2e9e814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "#LSTM\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        " \n",
        "torch.manual_seed(1)\n",
        " \n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    tensor = torch.LongTensor(idxs)\n",
        "    return Variable(tensor)\n",
        " \n",
        "training_data = [\n",
        "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "word_to_ix = {}\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        " \n",
        "char_to_ix = {}\n",
        "char_to_ix[' '] = len(char_to_ix)\n",
        "for sent, _ in training_data:\n",
        "    for word in sent:\n",
        "        for char in word:\n",
        "            if char not in char_to_ix:\n",
        "                char_to_ix[char] = len(char_to_ix)\n",
        " \n",
        "# print(char_to_ix)\n",
        "# print('len(char_to_ix):',len(char_to_ix))\n",
        "# print(word_to_ix)\n",
        "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
        " \n",
        " \n",
        " \n",
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, word_emb_dim, char_emb_dim, hidden_dim, vocab_size, tagset_size, char_size):\n",
        "        super(LSTMTagger,self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.char_emb_dim = char_emb_dim\n",
        " \n",
        " \n",
        "        self.word_embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.char_embedding = nn.Embedding(char_size, char_emb_dim)\n",
        "        self.char_lstm = nn.LSTM(char_emb_dim, char_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim + char_emb_dim, hidden_dim)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        " \n",
        "    def forward(self, sentence_word, sentence_char, MAX_WORD_LEN):\n",
        "        # char embeddings\n",
        "        sentence_size = sentence_word.size()[0]\n",
        "        char_emb = self.char_embedding(sentence_char) # [sentence_size * MAX_WORD_LEN, char_emb_dim]\n",
        "        try :\n",
        "            char_emb = char_emb.view(len(sentence_word), MAX_WORD_LEN, -1).permute(1,0,2) # [MAX_WORD_LEN, sentence_size, char_emb_dim]\n",
        "        except :\n",
        "            print(\"char_emb.size():\",char_emb.size())\n",
        " \n",
        "        self.hidden_char = self.initHidden_char(sentence_size)\n",
        "        char_lstm_out, self.hidden = self.char_lstm(char_emb, self.hidden_char)\n",
        "        char_embeded = char_lstm_out[-1,:,:].view(sentence_size,-1)\n",
        " \n",
        "        # word embeddings\n",
        "        word_embeded = self.word_embedding(sentence_word)\n",
        " \n",
        "        embeded = torch.cat((word_embeded, char_embeded),dim=1)\n",
        "        # print('embeded size:\\n', embeded.size())\n",
        "        self.hidden = self.initHidden()\n",
        "        lstm_out, self.hidden = self.lstm(embeded.view(sentence_size,1,-1), self.hidden)\n",
        "        tag_space = self.hidden2tag(lstm_out.view(sentence_size,-1))\n",
        "        tag_scores = F.log_softmax(tag_space)\n",
        "        return tag_scores\n",
        " \n",
        "    def initHidden(self):\n",
        "        result = (Variable(torch.zeros(1,1,self.hidden_dim)),\n",
        "                  Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
        "        return result\n",
        " \n",
        "    def initHidden_char(self, sentence_size):\n",
        "        result = (Variable(torch.zeros(1, sentence_size, self.char_emb_dim)),\n",
        "                  Variable(torch.zeros(1, sentence_size, self.char_emb_dim)))\n",
        "        return result\n",
        " \n",
        "# These will usually be more like 32 or 64 dimensional.\n",
        "# We will keep them small, so we can see how the weights change as we train.\n",
        "WORD_EMB_DIM = 5\n",
        "CHAR_EMB_DIM = 3\n",
        "HIDDEN_DIM = 5\n",
        "MAX_WORD_LEN = 8\n",
        " \n",
        "model = LSTMTagger(WORD_EMB_DIM, CHAR_EMB_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix), len(char_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        " \n",
        "# before training\n",
        "print('before training')\n",
        "sentence_word = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "sent_chars = []\n",
        "for w in training_data[0][0]:\n",
        "    sps = ' ' * (MAX_WORD_LEN - len(w))\n",
        "    sent_chars.extend(list(sps + w) if len(w) < MAX_WORD_LEN else list(w[:MAX_WORD_LEN]))\n",
        "sentence_char = prepare_sequence(sent_chars, char_to_ix)\n",
        " \n",
        "tag_scores = model(sentence_word, sentence_char, MAX_WORD_LEN)\n",
        "targets = prepare_sequence(training_data[0][1], tag_to_ix)\n",
        "print(tag_scores)\n",
        "print('targets:\\n',targets)\n",
        " \n",
        "for epoch in range(300):\n",
        "    for sentence, tags in training_data:\n",
        "        model.zero_grad()\n",
        "        model.hidden = model.initHidden()\n",
        "        sentence_word = prepare_sequence(sentence, word_to_ix)\n",
        "        sent_chars = []\n",
        "        for w in sentence:\n",
        "            sps = ' ' * (MAX_WORD_LEN - len(w))\n",
        "            sent_chars.extend(list(sps + w) if len(w)<MAX_WORD_LEN else list(w[:MAX_WORD_LEN]))\n",
        "        sentence_char = prepare_sequence(sent_chars, char_to_ix)\n",
        "        # sentence_char = prepare_char(sentence, char_to_ix, max_length=7)\n",
        " \n",
        "        targets = prepare_sequence(tags, tag_to_ix)\n",
        "        tag_scores = model(sentence_word, sentence_char, MAX_WORD_LEN)\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        " \n",
        "# after training\n",
        "print('after training')\n",
        "sentence_word = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "sent_chars = []\n",
        "for w in training_data[0][0]:\n",
        "    sps = ' ' * (MAX_WORD_LEN - len(w))\n",
        "    sent_chars.extend(list(sps + w) if len(w) < MAX_WORD_LEN else list(w[:MAX_WORD_LEN]))\n",
        "sentence_char = prepare_sequence(sent_chars, char_to_ix)\n",
        " \n",
        "tag_scores = model(sentence_word, sentence_char, MAX_WORD_LEN)\n",
        "targets = prepare_sequence(training_data[0][1], tag_to_ix)\n",
        "print(tag_scores)\n",
        "print('targets:\\n',targets)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:73: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.8570, -0.9973, -1.5765],\n",
            "        [-0.8414, -1.0452, -1.5267],\n",
            "        [-0.8354, -1.0456, -1.5381],\n",
            "        [-0.8176, -1.0923, -1.5002],\n",
            "        [-0.8011, -1.1269, -1.4822]], grad_fn=<LogSoftmaxBackward>)\n",
            "targets:\n",
            " tensor([0, 1, 2, 0, 1])\n",
            "after training\n",
            "tensor([[-0.0897, -2.5797, -4.6074],\n",
            "        [-3.4989, -0.0612, -3.5363],\n",
            "        [-2.7310, -2.5347, -0.1560],\n",
            "        [-0.5106, -1.7678, -1.4733],\n",
            "        [-2.3670, -0.2001, -2.4354]], grad_fn=<LogSoftmaxBackward>)\n",
            "targets:\n",
            " tensor([0, 1, 2, 0, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dipqjh_4BDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}